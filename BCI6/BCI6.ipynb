{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db6215f",
   "metadata": {},
   "source": [
    "task)\n",
    "\n",
    "1. CSP, FFT 이용하여 데이터 전처리\n",
    "2. 머신러닝 기법(로지스틱)으로 데이터 분류\n",
    "3. 전처리 데이터 fc layer에 넣어서 결과 확인하기(은닉층 3개 정도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3925f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: FutureWarning: mne.io.pick.pick_types is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:219: FutureWarning: mne.io.pick.pick_channels_regexp is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "<frozen importlib._bootstrap>:219: FutureWarning: mne.io.pick.channel_type is deprecated will be removed in 1.6, use documented public API instead. If no appropriate public API exists, please open an issue on GitHub.\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 8, 257)\n",
      "['NonTarget' 'NonTarget' 'NonTarget' 'NonTarget' 'NonTarget']\n",
      "   subject session run\n",
      "0        1       0   0\n",
      "1        1       0   0\n",
      "2        1       0   0\n",
      "3        1       0   0\n",
      "4        1       0   0\n"
     ]
    }
   ],
   "source": [
    "#moabb 데이터셋 불러오기\n",
    "\n",
    "from moabb.datasets import BNCI2014_008\n",
    "from moabb.paradigms import P300\n",
    "\n",
    "# 데이터셋과 패러다임 설정\n",
    "dataset = BNCI2014_008()\n",
    "paradigm = P300()  # MotorImagery() 대신 P300() 사용\n",
    "\n",
    "# 데이터 불러오기\n",
    "X, labels, metadata = paradigm.get_data(dataset)\n",
    "\n",
    "# 데이터 확인\n",
    "print(X.shape)\n",
    "print(labels[:5])\n",
    "print(metadata.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee0010",
   "metadata": {},
   "source": [
    "[CSP와 FFT를 이용하여 데이터 전처리]\n",
    "\n",
    "CSP 4개 특성, FFT 8개 특성 추출하여 총 12개 특성 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55abb5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (33600, 8, 257)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1e+02 (2.2e-16 eps * 8 dim * 5.8e+16  max singular value)\n",
      "    Estimated rank (mag): 8\n",
      "    MAG: rank 8 computed from 8 data channels with 0 projectors\n",
      "Reducing data rank from 8 -> 8\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 47 (2.2e-16 eps * 8 dim * 2.6e+16  max singular value)\n",
      "    Estimated rank (mag): 8\n",
      "    MAG: rank 8 computed from 8 data channels with 0 projectors\n",
      "Reducing data rank from 8 -> 8\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "After CSP: (33600, 4)\n",
      "After FFT: (33600, 8)\n",
      "Final processed shape: (33600, 12)\n"
     ]
    }
   ],
   "source": [
    "#csp와 fft를 적용하여 데이터 전처리\n",
    "\n",
    "import numpy as np\n",
    "from mne.decoding import CSP\n",
    "from scipy.fft import fft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ✅ 원본 데이터 (X: (samples, channels, timepoints))\n",
    "print(\"Original shape:\", X.shape)  # (4200, 8, 256)\n",
    "\n",
    "# 1️⃣ CSP 적용 (n_components=4)\n",
    "csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "X_csp = csp.fit_transform(X, labels)\n",
    "print(\"After CSP:\", X_csp.shape)  # (4200, 4)\n",
    "\n",
    "# 2️⃣ FFT 적용 (주파수 변환 후 평균)\n",
    "def compute_fft(X):\n",
    "    X_fft = np.abs(fft(X, axis=2))  # FFT 적용 후 절댓값\n",
    "    return X_fft.mean(axis=2)  # 시간축 평균 계산\n",
    "\n",
    "X_fft = compute_fft(X)\n",
    "print(\"After FFT:\", X_fft.shape)  # (4200, 8)\n",
    "\n",
    "# 3️⃣ 최종 데이터 (CSP + FFT 특징 결합)\n",
    "X_features = np.hstack((X_csp, X_fft))\n",
    "scaler = StandardScaler()\n",
    "X_features = scaler.fit_transform(X_features)\n",
    "\n",
    "print(\"Final processed shape:\", X_features.shape)  # (4200, 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bad48f",
   "metadata": {},
   "source": [
    "X_features : (4200, 12) → 분류 모델의 입력 데이터\n",
    "\n",
    "-> (샘플 수, CSP 특징 개수 + FFT 특징 개수)\n",
    "\n",
    "\n",
    "labels : (4200,) → 레이블 (0 = NT, 1 = T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[여러가지 이진분류 모델 사용하여 분류]\n",
    "\n",
    "전처리된 EEG 데이터가 어느 타깃 T(1), NT(0)에 속하는지 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66dbf48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Random Forest Accuracy: 0.8363\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.8366\n",
      "Training LDA...\n",
      "LDA Accuracy: 0.8366\n",
      "Training KNN...\n",
      "KNN Accuracy: 0.8107\n",
      "Training SVM...\n",
      "SVM Accuracy: 0.8366\n"
     ]
    }
   ],
   "source": [
    "#여러가지 이진분류 모델로 주어진 EEG 신호가 NT인지 T인지 이진분류\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ✅ 데이터셋 (CSP + FFT 특징)\n",
    "X = X_features  # 전처리된 특징 데이터 (CSP + FFT)\n",
    "labels = np.array(labels)  # 이진 레이블 (0: NT, 1: T)\n",
    "\n",
    "# ✅ Train-Test 데이터 분할\n",
    "X_train, X_test, labels_train, labels_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ 여러 분류 모델 정의\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"LDA\": LDA(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"SVM\": SVC(kernel='rbf', C=1, probability=True)\n",
    "}\n",
    "\n",
    "# ✅ 각 모델 학습 및 평가\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")  # 훈련 시작 메시지 출력\n",
    "    model.fit(X_train, labels_train)  # 모델 학습\n",
    "    labels_pred = model.predict(X_test)  # 예측 수행\n",
    "    accuracy = accuracy_score(labels_test, labels_pred)  # 정확도 계산\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")  # 정확도 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fa80a",
   "metadata": {},
   "source": [
    "#SVM\n",
    "\n",
    "support vector machine\n",
    "\n",
    "support : 데이터의 경계선\n",
    "\n",
    "vector : 직선\n",
    "\n",
    "'데이터 간의 경계들과 최대한 멀리 떨어져있는 선'을 찾는 것(각 데이터 경계와 선 사이의 여백이 최대가 되도록 하는 선을 찾는 것)\n",
    "\n",
    "다중분류 문제에서도 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c94c5",
   "metadata": {},
   "source": [
    "[FC layer 사용하여 분류]\n",
    "\n",
    "step2에서는 각종 머신러닝 이진분류 모델을 사용했고,\n",
    "\n",
    "이제는 여러 은닉층을 두는 '딥러닝' 으로 분류 수행\n",
    "\n",
    "3개의 은닉층으로 설계\n",
    "\n",
    "\n",
    "✅ Fully Connected (FC) 신경망 구조\n",
    "✅ 은닉층 3개 (ReLU 활성화 함수 사용)\n",
    "✅ 출력층 (이진 분류 → 시그모이드 활성화 함수 사용)\n",
    "✅ Adam 옵티마이저 + Binary Crossentropy 손실 함수 사용\n",
    "✅ Train/Test 분할 후 학습 및 평가 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cf20867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels after conversion: [0. 1.]\n",
      "Epoch 1/50\n",
      "1680/1680 [==============================] - 5s 3ms/step - loss: 0.4598 - accuracy: 0.8324 - val_loss: 0.4454 - val_accuracy: 0.8366\n",
      "Epoch 2/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4519 - accuracy: 0.8325 - val_loss: 0.4439 - val_accuracy: 0.8366\n",
      "Epoch 3/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4509 - accuracy: 0.8325 - val_loss: 0.4450 - val_accuracy: 0.8366\n",
      "Epoch 4/50\n",
      "1680/1680 [==============================] - 4s 3ms/step - loss: 0.4509 - accuracy: 0.8325 - val_loss: 0.4441 - val_accuracy: 0.8366\n",
      "Epoch 5/50\n",
      "1680/1680 [==============================] - 4s 3ms/step - loss: 0.4502 - accuracy: 0.8325 - val_loss: 0.4443 - val_accuracy: 0.8366\n",
      "Epoch 6/50\n",
      "1680/1680 [==============================] - 4s 3ms/step - loss: 0.4501 - accuracy: 0.8325 - val_loss: 0.4429 - val_accuracy: 0.8366\n",
      "Epoch 7/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4500 - accuracy: 0.8325 - val_loss: 0.4432 - val_accuracy: 0.8366\n",
      "Epoch 8/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4496 - accuracy: 0.8325 - val_loss: 0.4437 - val_accuracy: 0.8366\n",
      "Epoch 9/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4493 - accuracy: 0.8325 - val_loss: 0.4465 - val_accuracy: 0.8366\n",
      "Epoch 10/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4491 - accuracy: 0.8325 - val_loss: 0.4457 - val_accuracy: 0.8366\n",
      "Epoch 11/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4489 - accuracy: 0.8325 - val_loss: 0.4440 - val_accuracy: 0.8366\n",
      "Epoch 12/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4489 - accuracy: 0.8325 - val_loss: 0.4434 - val_accuracy: 0.8366\n",
      "Epoch 13/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4478 - accuracy: 0.8325 - val_loss: 0.4446 - val_accuracy: 0.8366\n",
      "Epoch 14/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4479 - accuracy: 0.8325 - val_loss: 0.4441 - val_accuracy: 0.8366\n",
      "Epoch 15/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4470 - accuracy: 0.8325 - val_loss: 0.4461 - val_accuracy: 0.8366\n",
      "Epoch 16/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4471 - accuracy: 0.8325 - val_loss: 0.4448 - val_accuracy: 0.8366\n",
      "Epoch 17/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4469 - accuracy: 0.8325 - val_loss: 0.4437 - val_accuracy: 0.8366\n",
      "Epoch 18/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4465 - accuracy: 0.8326 - val_loss: 0.4477 - val_accuracy: 0.8363\n",
      "Epoch 19/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4454 - accuracy: 0.8327 - val_loss: 0.4455 - val_accuracy: 0.8366\n",
      "Epoch 20/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4446 - accuracy: 0.8327 - val_loss: 0.4476 - val_accuracy: 0.8359\n",
      "Epoch 21/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4442 - accuracy: 0.8327 - val_loss: 0.4464 - val_accuracy: 0.8366\n",
      "Epoch 22/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4430 - accuracy: 0.8328 - val_loss: 0.4469 - val_accuracy: 0.8356\n",
      "Epoch 23/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4419 - accuracy: 0.8330 - val_loss: 0.4476 - val_accuracy: 0.8354\n",
      "Epoch 24/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4415 - accuracy: 0.8330 - val_loss: 0.4490 - val_accuracy: 0.8354\n",
      "Epoch 25/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4409 - accuracy: 0.8339 - val_loss: 0.4508 - val_accuracy: 0.8336\n",
      "Epoch 26/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4405 - accuracy: 0.8337 - val_loss: 0.4495 - val_accuracy: 0.8338\n",
      "Epoch 27/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4391 - accuracy: 0.8343 - val_loss: 0.4523 - val_accuracy: 0.8330\n",
      "Epoch 28/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4381 - accuracy: 0.8344 - val_loss: 0.4544 - val_accuracy: 0.8335\n",
      "Epoch 29/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4367 - accuracy: 0.8349 - val_loss: 0.4531 - val_accuracy: 0.8314\n",
      "Epoch 30/50\n",
      "1680/1680 [==============================] - 5s 3ms/step - loss: 0.4357 - accuracy: 0.8347 - val_loss: 0.4520 - val_accuracy: 0.8329\n",
      "Epoch 31/50\n",
      "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4339 - accuracy: 0.8356 - val_loss: 0.4537 - val_accuracy: 0.8323\n",
      "Epoch 32/50\n",
      "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4334 - accuracy: 0.8357 - val_loss: 0.4604 - val_accuracy: 0.8271\n",
      "Epoch 33/50\n",
      "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4324 - accuracy: 0.8367 - val_loss: 0.4614 - val_accuracy: 0.8284\n",
      "Epoch 34/50\n",
      "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4306 - accuracy: 0.8376 - val_loss: 0.4590 - val_accuracy: 0.8274\n",
      "Epoch 35/50\n",
      "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4294 - accuracy: 0.8372 - val_loss: 0.4591 - val_accuracy: 0.8301\n",
      "Epoch 36/50\n",
      "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4277 - accuracy: 0.8375 - val_loss: 0.4601 - val_accuracy: 0.8286\n",
      "Epoch 37/50\n",
      "1680/1680 [==============================] - 7s 4ms/step - loss: 0.4267 - accuracy: 0.8384 - val_loss: 0.4698 - val_accuracy: 0.8318\n",
      "Epoch 38/50\n",
      "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4259 - accuracy: 0.8391 - val_loss: 0.4666 - val_accuracy: 0.8295\n",
      "Epoch 39/50\n",
      "1680/1680 [==============================] - 7s 4ms/step - loss: 0.4235 - accuracy: 0.8397 - val_loss: 0.4731 - val_accuracy: 0.8280\n",
      "Epoch 40/50\n",
      "1680/1680 [==============================] - 7s 4ms/step - loss: 0.4234 - accuracy: 0.8390 - val_loss: 0.4724 - val_accuracy: 0.8299\n",
      "Epoch 41/50\n",
      "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4217 - accuracy: 0.8398 - val_loss: 0.4698 - val_accuracy: 0.8293\n",
      "Epoch 42/50\n",
      "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4188 - accuracy: 0.8403 - val_loss: 0.4805 - val_accuracy: 0.8290\n",
      "Epoch 43/50\n",
      "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4190 - accuracy: 0.8400 - val_loss: 0.4806 - val_accuracy: 0.8289\n",
      "Epoch 44/50\n",
      "1680/1680 [==============================] - 5s 3ms/step - loss: 0.4163 - accuracy: 0.8414 - val_loss: 0.4802 - val_accuracy: 0.8287\n",
      "Epoch 45/50\n",
      "1680/1680 [==============================] - 5s 3ms/step - loss: 0.4130 - accuracy: 0.8428 - val_loss: 0.4886 - val_accuracy: 0.8283\n",
      "Epoch 46/50\n",
      "1680/1680 [==============================] - 5s 3ms/step - loss: 0.4135 - accuracy: 0.8422 - val_loss: 0.4911 - val_accuracy: 0.8271\n",
      "Epoch 47/50\n",
      "1680/1680 [==============================] - 5s 3ms/step - loss: 0.4123 - accuracy: 0.8436 - val_loss: 0.5113 - val_accuracy: 0.8235\n",
      "Epoch 48/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4123 - accuracy: 0.8430 - val_loss: 0.5033 - val_accuracy: 0.8225\n",
      "Epoch 49/50\n",
      "1680/1680 [==============================] - 4s 3ms/step - loss: 0.4100 - accuracy: 0.8440 - val_loss: 0.5033 - val_accuracy: 0.8259\n",
      "Epoch 50/50\n",
      "1680/1680 [==============================] - 4s 2ms/step - loss: 0.4074 - accuracy: 0.8448 - val_loss: 0.4991 - val_accuracy: 0.8231\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.8231\n",
      "Test Accuracy: 0.8231\n",
      "Current Learning Rate: 0.0010000000474974513\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ✅ 데이터 준비 (CSP + FFT 특징)\n",
    "X = X_features  # 전처리된 특징 데이터\n",
    "\n",
    "# ✅ 문자열 라벨을 0과 1로 변환 (Target → 1, NonTarget → 0)\n",
    "y = np.array([1 if label == \"Target\" else 0 for label in labels], dtype=np.float32)\n",
    "\n",
    "# ✅ 변환 확인\n",
    "print(\"Unique labels after conversion:\", np.unique(y))  # [0. 1.]이 나와야 정상\n",
    "\n",
    "# ✅ 데이터 정규화 (표준화)\n",
    "scaler = StandardScaler()\n",
    "X= scaler.fit_transform(X)\n",
    "\n",
    "# ✅ Train-Test 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ FC Layer 기반 모델 생성\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(512, activation='relu', input_shape=(X.shape[1],)),  # 첫 번째 은닉층\n",
    "    keras.layers.Dense(256, activation='relu'),  # 두 번째 은닉층\n",
    "    keras.layers.Dense(128, activation='relu'),  # 세 번째 은닉층\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # 출력층 (이진 분류 → sigmoid)\n",
    "])\n",
    "\n",
    "# ✅ 모델 컴파일 (Binary Crossentropy, Adam 옵티마이저)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ✅ 모델 학습\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# ✅ 최종 정확도 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#학습률 확인\n",
    "learning_rate = model.optimizer.learning_rate.numpy()\n",
    "print(f\"Current Learning Rate: {learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fd2c8",
   "metadata": {},
   "source": [
    "특성 12개로 학습한 머신러닝 모델의 최고 성능 : 0.8366\n",
    "특성 12개로 학습한 딥러닝 모델의 최고 성능 : 0.8366\n",
    "\n",
    "\n",
    "\n",
    "원본 데이터 shape (33600, 8, 257)\n",
    "-> 모델이 학습할 수 있는 특성의 개수 8x257개(feature 수)\n",
    "\n",
    "근데 이제\n",
    "\n",
    "csp, fft를 이용해서 특성을 4 + 8 = 12개로 줄여버림\n",
    "\n",
    "사실상 특성이 2056개에서 12개로 줄어든 것\n",
    "\n",
    "\n",
    "여러 이진분류 모델을 사용한 step2에서는 머신러닝을 사용했기 때문에 12개의 특성이 부족하지 않을 수 있지만,\n",
    "\n",
    "step3에서 fc layer을 이용한 딥러닝 모델을 학습할 경우, 12개의 특성은 너무너무 작은 개수임\n",
    "\n",
    "따라서, 이제부터는 csp, fft를 이용하여 특성의 개수를 제한하지 않고, \n",
    "단순히 원본 데이터를 flatten하여 fc layer에 전달하는 방식으로 충분한 특성 수를 보장하여 딥러닝 모델을 학습시킬 것임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ea637fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n",
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 8, 257)\n",
      "['NonTarget' 'NonTarget' 'NonTarget' 'NonTarget' 'NonTarget']\n",
      "   subject session run\n",
      "0        1       0   0\n",
      "1        1       0   0\n",
      "2        1       0   0\n",
      "3        1       0   0\n",
      "4        1       0   0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangm\\anaconda3\\envs\\moabb\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  return func(X, **(kw_args if kw_args else {}))\n"
     ]
    }
   ],
   "source": [
    "#CSP, FFT 를 사용하지 않고 단순 flatten한 데이터 사용\n",
    "\n",
    "#moabb 데이터셋 불러오기\n",
    "\n",
    "from moabb.datasets import BNCI2014_008\n",
    "from moabb.paradigms import P300\n",
    "\n",
    "# 데이터셋과 패러다임 설정\n",
    "dataset = BNCI2014_008()\n",
    "paradigm = P300()  # MotorImagery() 대신 P300() 사용\n",
    "\n",
    "# 데이터 불러오기\n",
    "origin, labels, metadata = paradigm.get_data(dataset)\n",
    "\n",
    "# 데이터 확인\n",
    "print(origin.shape)\n",
    "print(labels[:5])\n",
    "print(metadata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b33367a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (33600, 8, 257)\n",
      "Unique labels: [0. 1.]\n",
      "Flattened X shape: (33600, 2056)\n",
      "Epoch 1/50\n",
      "840/840 [==============================] - 34s 39ms/step - loss: 0.5129 - accuracy: 0.7918 - val_loss: 0.4151 - val_accuracy: 0.8269\n",
      "Epoch 2/50\n",
      "840/840 [==============================] - 34s 40ms/step - loss: 0.3991 - accuracy: 0.8376 - val_loss: 0.3831 - val_accuracy: 0.8423\n",
      "Epoch 3/50\n",
      "840/840 [==============================] - 45s 53ms/step - loss: 0.3752 - accuracy: 0.8455 - val_loss: 0.4003 - val_accuracy: 0.8345\n",
      "Epoch 4/50\n",
      "840/840 [==============================] - 58s 68ms/step - loss: 0.3607 - accuracy: 0.8496 - val_loss: 0.3848 - val_accuracy: 0.8418\n",
      "Epoch 5/50\n",
      "840/840 [==============================] - 69s 82ms/step - loss: 0.3493 - accuracy: 0.8531 - val_loss: 0.3829 - val_accuracy: 0.8454\n",
      "Epoch 6/50\n",
      "840/840 [==============================] - 61s 73ms/step - loss: 0.3417 - accuracy: 0.8570 - val_loss: 0.3797 - val_accuracy: 0.8452\n",
      "Epoch 7/50\n",
      "840/840 [==============================] - 63s 75ms/step - loss: 0.3296 - accuracy: 0.8606 - val_loss: 0.3711 - val_accuracy: 0.8521\n",
      "Epoch 8/50\n",
      "840/840 [==============================] - 57s 68ms/step - loss: 0.3207 - accuracy: 0.8651 - val_loss: 0.3801 - val_accuracy: 0.8497\n",
      "Epoch 9/50\n",
      "840/840 [==============================] - 63s 75ms/step - loss: 0.3085 - accuracy: 0.8733 - val_loss: 0.3909 - val_accuracy: 0.8461\n",
      "Epoch 10/50\n",
      "840/840 [==============================] - 54s 65ms/step - loss: 0.2920 - accuracy: 0.8774 - val_loss: 0.4021 - val_accuracy: 0.8438\n",
      "Epoch 11/50\n",
      "840/840 [==============================] - 62s 74ms/step - loss: 0.2771 - accuracy: 0.8852 - val_loss: 0.3861 - val_accuracy: 0.8472\n",
      "Epoch 12/50\n",
      "840/840 [==============================] - 56s 67ms/step - loss: 0.2621 - accuracy: 0.8900 - val_loss: 0.4296 - val_accuracy: 0.8381\n",
      "Epoch 13/50\n",
      "840/840 [==============================] - 63s 75ms/step - loss: 0.2444 - accuracy: 0.8973 - val_loss: 0.4628 - val_accuracy: 0.8324\n",
      "Epoch 14/50\n",
      "840/840 [==============================] - 61s 73ms/step - loss: 0.2292 - accuracy: 0.9054 - val_loss: 0.4502 - val_accuracy: 0.8375\n",
      "Epoch 15/50\n",
      "840/840 [==============================] - 65s 78ms/step - loss: 0.2125 - accuracy: 0.9139 - val_loss: 0.4689 - val_accuracy: 0.8341\n",
      "Epoch 16/50\n",
      "840/840 [==============================] - 62s 73ms/step - loss: 0.2002 - accuracy: 0.9171 - val_loss: 0.4759 - val_accuracy: 0.8399\n",
      "Epoch 17/50\n",
      "840/840 [==============================] - 64s 76ms/step - loss: 0.1863 - accuracy: 0.9241 - val_loss: 0.5092 - val_accuracy: 0.8384\n",
      "Epoch 18/50\n",
      "840/840 [==============================] - 63s 75ms/step - loss: 0.1710 - accuracy: 0.9297 - val_loss: 0.5558 - val_accuracy: 0.8290\n",
      "Epoch 19/50\n",
      "840/840 [==============================] - 59s 70ms/step - loss: 0.1626 - accuracy: 0.9342 - val_loss: 0.5315 - val_accuracy: 0.8314\n",
      "Epoch 20/50\n",
      "840/840 [==============================] - 63s 75ms/step - loss: 0.1510 - accuracy: 0.9397 - val_loss: 0.5677 - val_accuracy: 0.8238\n",
      "Epoch 21/50\n",
      "840/840 [==============================] - 60s 71ms/step - loss: 0.1433 - accuracy: 0.9421 - val_loss: 0.5833 - val_accuracy: 0.8272\n",
      "Epoch 22/50\n",
      "840/840 [==============================] - 77s 92ms/step - loss: 0.1349 - accuracy: 0.9471 - val_loss: 0.6196 - val_accuracy: 0.8192\n",
      "Epoch 23/50\n",
      "840/840 [==============================] - 81s 96ms/step - loss: 0.1320 - accuracy: 0.9493 - val_loss: 0.6179 - val_accuracy: 0.8295\n",
      "Epoch 24/50\n",
      "840/840 [==============================] - 81s 96ms/step - loss: 0.1230 - accuracy: 0.9514 - val_loss: 0.6356 - val_accuracy: 0.8272\n",
      "Epoch 25/50\n",
      "840/840 [==============================] - 81s 96ms/step - loss: 0.1190 - accuracy: 0.9529 - val_loss: 0.6568 - val_accuracy: 0.8304\n",
      "Epoch 26/50\n",
      "840/840 [==============================] - 81s 97ms/step - loss: 0.1165 - accuracy: 0.9540 - val_loss: 0.6430 - val_accuracy: 0.8287\n",
      "Epoch 27/50\n",
      "840/840 [==============================] - 80s 95ms/step - loss: 0.1039 - accuracy: 0.9606 - val_loss: 0.7064 - val_accuracy: 0.8311\n",
      "Epoch 28/50\n",
      "840/840 [==============================] - 84s 100ms/step - loss: 0.1077 - accuracy: 0.9575 - val_loss: 0.6642 - val_accuracy: 0.8293\n",
      "Epoch 29/50\n",
      "840/840 [==============================] - 84s 100ms/step - loss: 0.0950 - accuracy: 0.9638 - val_loss: 0.7278 - val_accuracy: 0.8296\n",
      "Epoch 30/50\n",
      "840/840 [==============================] - 81s 96ms/step - loss: 0.0988 - accuracy: 0.9603 - val_loss: 0.7346 - val_accuracy: 0.8269\n",
      "Epoch 31/50\n",
      "840/840 [==============================] - 79s 95ms/step - loss: 0.0941 - accuracy: 0.9641 - val_loss: 0.7294 - val_accuracy: 0.8226\n",
      "Epoch 32/50\n",
      "840/840 [==============================] - 81s 97ms/step - loss: 0.0929 - accuracy: 0.9646 - val_loss: 0.7331 - val_accuracy: 0.8281\n",
      "Epoch 33/50\n",
      "840/840 [==============================] - 83s 99ms/step - loss: 0.0925 - accuracy: 0.9638 - val_loss: 0.7039 - val_accuracy: 0.8253\n",
      "Epoch 34/50\n",
      "840/840 [==============================] - 81s 97ms/step - loss: 0.0882 - accuracy: 0.9669 - val_loss: 0.7741 - val_accuracy: 0.8287\n",
      "Epoch 35/50\n",
      "840/840 [==============================] - 81s 96ms/step - loss: 0.0843 - accuracy: 0.9672 - val_loss: 0.7458 - val_accuracy: 0.8275\n",
      "Epoch 36/50\n",
      "840/840 [==============================] - 82s 98ms/step - loss: 0.0778 - accuracy: 0.9714 - val_loss: 0.7855 - val_accuracy: 0.8222\n",
      "Epoch 37/50\n",
      "840/840 [==============================] - 85s 101ms/step - loss: 0.0776 - accuracy: 0.9712 - val_loss: 0.7896 - val_accuracy: 0.8284\n",
      "Epoch 38/50\n",
      "840/840 [==============================] - 108s 128ms/step - loss: 0.0793 - accuracy: 0.9698 - val_loss: 0.7918 - val_accuracy: 0.8268\n",
      "Epoch 39/50\n",
      "840/840 [==============================] - 125s 149ms/step - loss: 0.0752 - accuracy: 0.9708 - val_loss: 0.8361 - val_accuracy: 0.8305\n",
      "Epoch 40/50\n",
      "840/840 [==============================] - 87s 104ms/step - loss: 0.0691 - accuracy: 0.9747 - val_loss: 0.8122 - val_accuracy: 0.8271\n",
      "Epoch 41/50\n",
      "840/840 [==============================] - 90s 107ms/step - loss: 0.0760 - accuracy: 0.9709 - val_loss: 0.8120 - val_accuracy: 0.8283\n",
      "Epoch 42/50\n",
      "840/840 [==============================] - 90s 107ms/step - loss: 0.0715 - accuracy: 0.9728 - val_loss: 0.8234 - val_accuracy: 0.8292\n",
      "Epoch 43/50\n",
      "840/840 [==============================] - 85s 101ms/step - loss: 0.0676 - accuracy: 0.9755 - val_loss: 0.8679 - val_accuracy: 0.8152\n",
      "Epoch 44/50\n",
      "840/840 [==============================] - 79s 95ms/step - loss: 0.0677 - accuracy: 0.9748 - val_loss: 0.8674 - val_accuracy: 0.8222\n",
      "Epoch 45/50\n",
      "840/840 [==============================] - 81s 96ms/step - loss: 0.0693 - accuracy: 0.9737 - val_loss: 0.8556 - val_accuracy: 0.8241\n",
      "Epoch 46/50\n",
      "840/840 [==============================] - 81s 97ms/step - loss: 0.0612 - accuracy: 0.9776 - val_loss: 0.9067 - val_accuracy: 0.8251\n",
      "Epoch 47/50\n",
      "840/840 [==============================] - 83s 99ms/step - loss: 0.0659 - accuracy: 0.9758 - val_loss: 0.9209 - val_accuracy: 0.8196\n",
      "Epoch 48/50\n",
      "840/840 [==============================] - 81s 96ms/step - loss: 0.0659 - accuracy: 0.9756 - val_loss: 0.8563 - val_accuracy: 0.8333\n",
      "Epoch 49/50\n",
      "840/840 [==============================] - 80s 95ms/step - loss: 0.0607 - accuracy: 0.9765 - val_loss: 0.8947 - val_accuracy: 0.8228\n",
      "Epoch 50/50\n",
      "840/840 [==============================] - 82s 97ms/step - loss: 0.0633 - accuracy: 0.9762 - val_loss: 0.9197 - val_accuracy: 0.8290\n",
      "210/210 [==============================] - 3s 15ms/step - loss: 0.9197 - accuracy: 0.8290\n",
      "Test Accuracy: 0.8290\n",
      "Current Learning Rate: 0.0010000000474974513\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ✅ 원본 EEG 데이터 사용 (CSP & FFT 없이)\n",
    "X = np.array(origin)  # 원본 EEG 데이터 (shape: samples, channels, timepoints)\n",
    "y = np.array([1 if label == \"Target\" else 0 for label in labels], dtype=np.float32)  # 이진 라벨 변환\n",
    "\n",
    "# ✅ 데이터 확인\n",
    "print(f\"Original X shape: {X.shape}\")  # (샘플 수, 채널 수, 시간 포인트 수)\n",
    "print(f\"Unique labels: {np.unique(y)}\")  # [0. 1.]\n",
    "\n",
    "# ✅ Flatten 적용 (2D 벡터로 변환)\n",
    "X = X.reshape(X.shape[0], -1)  # (샘플 수, 채널 수 * 시간 포인트 수)\n",
    "print(f\"Flattened X shape: {X.shape}\")  # (샘플 수, 전체 특성 수)\n",
    "\n",
    "# ✅ 데이터 정규화 (표준화)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# ✅ Train-Test 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ FC Layer 기반 모델 생성\n",
    "# 결합되는 채널 수를 임의로 늘림(모델을 더 복잡하게 만듦)\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(2048, activation='relu', input_shape=(X.shape[1],)),  # 첫 번째 은닉층\n",
    "    keras.layers.BatchNormalization(),  # 배치 정규화 추가\n",
    "    keras.layers.Dropout(0.3),  # 과적합 방지\n",
    "    \n",
    "    keras.layers.Dense(1024, activation='relu'),  # 두 번째 은닉층\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512, activation='relu'),  # 세 번째 은닉층\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(1, activation='sigmoid')  # 출력층 (이진 분류 → sigmoid)\n",
    "])\n",
    "\n",
    "# ✅ 모델 컴파일 (Binary Crossentropy, Adam 옵티마이저)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ✅ 모델 학습\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# ✅ 최종 정확도 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#학습률 확인\n",
    "learning_rate = model.optimizer.learning_rate.numpy()\n",
    "print(f\"Current Learning Rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291d0e8",
   "metadata": {},
   "source": [
    "(훈련/검증 세트) 2개로 나눔\n",
    "\n",
    "\n",
    "에포크 7에서 검증 세트 점수 최대치(85.21%)를 찍음\n",
    "\n",
    "\n",
    "에포크8 이후부터 검증세트 점수가 떨어지고, 훈련세트 점수는 급격히 오름\n",
    "\n",
    "-> 이를 통해 에포크 7에서 최고 성능 85.21%를 찍고, 이후 에포크부터 점점 모델이 과대적합 됨을 알 수 있음\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "조정 가능한 하이퍼파라미터)\n",
    "\n",
    "- dropout\n",
    "- 뉴런수\n",
    "- 학습률(잘 학습되는 것을 보니, 조정 안하는게 좋을듯)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
